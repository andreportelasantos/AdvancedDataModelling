{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenising Text\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "1. **Tokenising strings** - splitting it into tokens (words etc.)\n",
    "2. **Alternative tokenizers**: ``regexp_tokenize, wordpunct_tokenize, blankline_tokenize,TweetTokenizer``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions & Objectives:\n",
    "\n",
    "- What is tokenisation?\n",
    "- How can a string of raw text be tokenised?\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- Tokenisation means to split a string into separate words and punctuation, for example to be able to count them.\n",
    "- Text can be tokenised using the a tokeniser, e.g. the punkt tokeniser in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process text we need to break it down into tokens. As we explained at the start, a token is a letter, word, number, or punctuation which is contained in a string.\n",
    "\n",
    "To tokenise we first need to import the word_tokenize method from the tokenize package from NLTK which allows us to do this without writing the code ourselves.\n",
    "\n",
    "We will also download a specific tokeniser that NLTK uses as default. There are different ways of tokenising text and today we will use NLTKâ€™s in-built punkt tokeniser by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asantos2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell. (it's fine if you see some pink warnings underneath it)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenise (split into tokens) a nursery rhyme \"Humpty Dumpty\".\n",
    "\n",
    "We will save the tokenised output in a list using the `humpty_tokens` variable and can inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Humpty', 'Dumpty', 'sat', 'on', 'a', 'wall', ',', 'Humpty', 'Dumpty', 'had', 'a', 'great', 'fall', ';', 'All', 'the', 'king', \"'s\", 'horses', 'and', 'all', 'the', 'king', \"'s\", 'men', 'could', \"n't\", 'put', 'Humpty', 'together', 'again', '.']\n"
     ]
    }
   ],
   "source": [
    "humpty_string = \"Humpty Dumpty sat on a wall, Humpty Dumpty had a great fall; All the king's horses and all the king's men couldn't put Humpty together again.\"\n",
    "humpty_tokens = word_tokenize(humpty_string)\n",
    "print(humpty_tokens) # print all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Humpty', 'Dumpty', 'sat', 'on', 'a', 'wall', ',', 'Humpty', 'Dumpty', 'had']\n"
     ]
    }
   ],
   "source": [
    "# let's print just a few of them to have a closer look:\n",
    "print(humpty_tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unifying and cleaning up the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyse the data, we'll first learn how to perforn some cleanup tasks. \n",
    "\n",
    "As you can see in the above example, some of the words are uppercase and some are lowercase. But Python is case-sensitive, which means that 'hope' and 'Hope' are considered to be two completely different strings.\n",
    "\n",
    "For example, when searching for a word or counting the occurrences of a word, we most likely will want to seek both for the lowercase and uppercase versions of the word (eg. `company` and `Company` ). That's why simplify the analysis, we often normalise the data and make it all lowercase. This way both of the above words would simply become `company` and will make the text easier to comprehend.\n",
    "\n",
    "Since our list of tokens is basically a list of strings (words) we can apply the `List comprehention Loop` we learned about before to transform our list of mixed-case words, into a list of lower-case words. \n",
    "\n",
    "As you might remember a syntax for such loop is `[output_format for item in items ]` where:\n",
    "\n",
    "- 'output_format' is some operation we perform on item, like `item.lower()` or `len(item)`\n",
    "- 'items' is the List with all the elements we want to transform\n",
    "- 'item' is a temporary name we give to each element of items, for the purposes of using that name inside of output_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify above example, so that we only aquire lowe-case tokens of the nursery rhyme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['humpty', 'dumpty', 'sat', 'on', 'a', 'wall', ',', 'humpty', 'dumpty', 'had', 'a', 'great', 'fall', ';', 'all', 'the', 'king', \"'s\", 'horses', 'and', 'all', 'the', 'king', \"'s\", 'men', 'could', \"n't\", 'put', 'humpty', 'together', 'again', '.']\n"
     ]
    }
   ],
   "source": [
    "humpty_string = \"Humpty Dumpty sat on a wall, Humpty Dumpty had a great fall; All the king's horses and all the king's men couldn't put Humpty together again.\"\n",
    "humpty_tokens = word_tokenize(humpty_string)\n",
    "lowercase_tokens = [token.lower() for token in humpty_tokens]\n",
    "print(lowercase_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative tokenizers\n",
    "\n",
    "Here we only show some simple examples for popular tokenizers.\n",
    "\n",
    "For more detailed use of tokenizers, please check the `nltk.tokenize` documentation website:\n",
    "    https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "print(tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize,TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `RegexpTokenizer` splits a string into substrings using a regular expression. For example, the following tokenizer forms tokens out of alphabetic sequences, money expressions, and any other non-whitespace sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(s, pattern='\\w+|\\$[\\d\\.]+|\\S+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wordpunct_tokenize` tokenizes a text into a sequence of alphabetic and non-alphabetic characters, using the regexp \\w+|[^\\w\\s]+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`blankline_tokenize` tokenizes a string, treating any sequence of blank lines as a delimiter. Blank lines are defined as lines containing no characters, except for space or tab characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.', 'Thanks.']\n"
     ]
    }
   ],
   "source": [
    "print(blankline_tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TweetTokenizer`: Twitter-aware tokenizer, designed to be flexible and easy to adapt to new domains and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@Xyd_', 'thank', 'you', 'for', 'always', 'taking', 'care', 'of', '@xyz', '.', 'Take', 'care', 'of', 'yourself', 'too', '!', 'Merry', 'Christmas', 'and', 'Happy', 'New', 'Year', 'Snowman', 'Christmas', 'treeSparkles', '.', '#MerryChristmas', '#HappyNewYear']\n",
      "['@Xyd_', 'thank', 'you', 'for', 'always', 'taking', 'care', 'of', '@xyz.', 'Take', 'care', 'of', 'yourself', 'too!', 'Merry', 'Christmas', 'and', 'Happy', 'New', 'Year', 'Snowman', 'Christmas', 'treeSparkles.', '#MerryChristmas', '#HappyNewYear']\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "s = '@Xyd_ thank you for always taking care of @xyz. Take care of yourself too! Merry Christmas and Happy New Year Snowman Christmas treeSparkles. #MerryChristmas #HappyNewYear'\n",
    "print(tknzr.tokenize(s))\n",
    "print(s.split()) #check their differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: ls_Text and Data Mining Bootcamp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
