{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, LSTMs are going to be used to predict the label (e.g. sentiment) of a sequence.\n",
    "\n",
    "We are going to use `keras` to build LSTM network, using function `keras.layers.LSTM`. First, let's install the library `tensorflow` and `keras`. This may take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install in Anaconda command: conda install -c conda-forge keras\n",
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imdb dataset: https://keras.io/api/datasets/imdb/#getwordindex-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000 # use top max_features most common words to build a vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data (and reducing its size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = x_train[:1000]\n",
    "x_test = x_test[:1000]\n",
    "y_train = y_train[:1000]\n",
    "y_test = y_test[:1000]\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to give you an idea of what the sequences look like (each number represents a different word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"X-vector: \"+str(x_train[0]))\n",
    "print(\"Label: \"+str(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your curiosity, here we just show how to retrieve the dictionary mapping word indices back to words.\n",
    "For more details, see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FROM=3   # word index offset, by default\n",
    "\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2 #unknown words according to the vovabulary\n",
    "word_to_id[\"<UNUSED>\"] = 3\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in x_train[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since sequences (in this case sentences) can have different lengths, we need to make sure that they are padded: we add zeros to the beginning of the sequences that are shorter than the longest sequence so we can still train them step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure sequences have same length\n",
    "maxlen = 80  # in each sentence, cut texts  before this number of words\n",
    "\n",
    "print('Transform sequences')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X-vector: \"+str(x_train[0]))\n",
    "print(\"Label: \"+str(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "When directly working with text, we need an embedding layer, where words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n",
    "Look at https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/ for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "no_dim = 128\n",
    "\n",
    "# First we create an embedding for each word of dimensionality 128\n",
    "# no_dim - should match LSTM\n",
    "model.add(Embedding(max_features, no_dim))\n",
    "\n",
    "# dropout = percentage of units dropped by the input linear transformation\n",
    "# rec_drop = percentage of units dropped by linear transformation of recurrent state\n",
    "model.add(LSTM(no_dim, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# dimensionality of the output space = 1: since we use classification of a label, e.g., [0,1,2,3]\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy','mae'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation happens as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(x_test, y_test,return_dict = True)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may check `keras.layers.LSTM`'s documentation for more details: \n",
    "https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "Or this Youtube tutorial video imdb classification using  `LSTM`\n",
    "https://www.youtube.com/watch?v=95F26zyK-c4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
