{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modelling with LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be using LSTMs for language modelling by learning sequences of sentences (windows) that lead up to a certain word.\n",
    "\n",
    "Note:\n",
    "first time running, you will need to install ``tensorflow`` and ``keras``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\andre\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.1)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorflow) (0.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\andre\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# first time running, run the following comment to install tenserflow\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\andre\\anaconda3\\lib\\site-packages (2.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\andre\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# first time running, run the following comment to install keras\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Emma by Jane Austen from the Gutenberg corpus again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_sents = nltk.corpus.gutenberg.sents('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ['CHAPTER', 'I'], ['Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.'], ['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.']]\n"
     ]
    }
   ],
   "source": [
    "# print the first 5 sentences\n",
    "print(emma_sents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we store all the words as integers (like a dictionary, similar to the one-hot vector):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']']\n",
      "[1, 2, 3, 4, 5, 6, 7]\n",
      "['VOLUME', 'I']\n",
      "[8, 9]\n",
      "['CHAPTER', 'I']\n",
      "[10, 9]\n",
      "['Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.']\n",
      "[2, 11, 12, 13, 12, 14, 12, 15, 16, 12, 17, 18, 19, 20, 15, 21, 22, 12, 23, 24, 25, 26, 27, 28, 29, 30, 27, 31, 32, 15, 33, 34, 35, 36, 37, 38, 39, 40, 28, 41, 17, 42, 43, 24, 44, 45, 46, 47, 48]\n",
      "['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.']\n",
      "[49, 50, 28, 51, 27, 28, 52, 53, 27, 18, 54, 55, 12, 56, 57, 32, 15, 33, 12, 40, 58, 27, 47, 59, 60, 61, 62, 12, 63, 64, 27, 65, 66, 67, 18, 42, 68, 69, 48]\n"
     ]
    }
   ],
   "source": [
    "# stores the words -> integers\n",
    "word_dict = {}\n",
    "# stores the integers -> words\n",
    "reverse_dict = {}\n",
    "\n",
    "# keeps track of the words\n",
    "no_words = 1\n",
    "\n",
    "int_sents = []\n",
    "longest = 0\n",
    "for i, sentence in enumerate(emma_sents[:1000]):\n",
    "    ints = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word not in word_dict.keys():\n",
    "            word_dict[word] = no_words\n",
    "            reverse_dict[no_words] = word\n",
    "            no_words += 1\n",
    "        # add word to the integer list\n",
    "        ints.append(word_dict[word])\n",
    "    \n",
    "    # Print to illustrate the conversion\n",
    "    if i<5:\n",
    "        print(sentence)\n",
    "        print(ints)\n",
    "    # Keep track of the longest sentence\n",
    "    if len(sentence) > longest:\n",
    "        longest = len(sentence)\n",
    "        \n",
    "    # store the integer sentence\n",
    "    int_sents.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Words:  2728\n",
      "#Sentences:  1000\n",
      "Length:  172\n"
     ]
    }
   ],
   "source": [
    "print('#Words: ',len(word_dict))\n",
    "print('#Sentences: ',len(int_sents))\n",
    "print('Length: ', longest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have to pad our sequences that are not long enough, adding 0's at the begining of each ``int_sents`` (although not as necessary in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sents = sequence.pad_sequences(int_sents, maxlen=longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "       10,  9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example\n",
    "int_sents[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider each input vector contains 30 words, and the corresponding output is the 31st word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is [1, 2, 3]\n",
      "output is 4\n"
     ]
    }
   ],
   "source": [
    "tt = [0,1,2,3,4,5,6]\n",
    "print('input is', tt[1:1+3])\n",
    "print('output is',tt[1+3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "window = 30\n",
    "\n",
    "# Go through the sentences and store the window (sequence of words) as X\n",
    "# y contains the word at the end of the window\n",
    "for sent in int_sents:\n",
    "    for i in range(0, len(sent) - window, 1):\n",
    "        window_x = sent[i:i + window]\n",
    "        window_y = sent[i + window]\n",
    "        X.append(window_x)\n",
    "        y.append(window_y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is [  0   0   0   0   0   0   0 145 166  12 167  12  50 168 169 170 171  12\n",
      " 172 129 173 174   3 121 175 176  88 177  17  47]\n",
      "output is 48\n"
     ]
    }
   ],
   "source": [
    "# For example\n",
    "sent = int_sents[10]\n",
    "i= len(sent) - window -1 #since range() creates until len(sent) - window -1\n",
    "print('input is', sent[i:i + window])\n",
    "print('output is',sent[i + window])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to store our data in the correct shape (#sentences x length window x #features (1 - the words)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142000, 30, 1)\n",
      "(142000, 2729)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(X, (len(X), window, 1))\n",
    "\n",
    "# For the y-value, we use one-hot encoding\n",
    "y = np_utils.to_categorical(y)    \n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [681]\n",
      " [578]\n",
      " [157]\n",
      " [332]\n",
      " [ 27]\n",
      " [136]\n",
      " [682]\n",
      " [ 27]\n",
      " [680]\n",
      " [490]\n",
      " [334]\n",
      " [102]\n",
      " [254]\n",
      " [683]\n",
      " [ 12]\n",
      " [  9]\n",
      " [ 77]\n",
      " [ 63]\n",
      " [ 40]\n",
      " [209]\n",
      " [684]\n",
      " [ 17]\n",
      " [501]\n",
      " [685]\n",
      " [ 32]\n",
      " [103]]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# For example, although many vectors are 0's since we only consider a dictionary of size 30\n",
    "ind = 12345\n",
    "print(X[ind])\n",
    "print(y[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:2000]\n",
    "y = y[:2000]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660, 2729)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model:\n",
    "\n",
    "Note:\n",
    "\n",
    "- check ``keras.LSTM`` configurations here https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "\n",
    "- ``Dense`` implements the operation: ``output = activation(dot(input, kernel) + bias)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 2s 94ms/step - loss: 7.8807 - accuracy: 0.8069 - mae: 7.3259e-04 - val_loss: 7.7964 - val_accuracy: 0.7910 - val_mae: 7.3256e-04\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 7.5048 - accuracy: 0.8134 - mae: 7.3244e-04 - val_loss: 6.3587 - val_accuracy: 0.7948 - val_mae: 7.3144e-04\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 5.5951 - accuracy: 0.8162 - mae: 7.2906e-04 - val_loss: 4.5831 - val_accuracy: 0.7985 - val_mae: 7.2083e-04\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 3.6749 - accuracy: 0.8134 - mae: 6.9333e-04 - val_loss: 2.6007 - val_accuracy: 0.7948 - val_mae: 5.9401e-04\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 1.9311 - accuracy: 0.8134 - mae: 4.6644e-04 - val_loss: 1.6454 - val_accuracy: 0.7948 - val_mae: 2.8577e-04\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 1.3088 - accuracy: 0.8116 - mae: 2.3240e-04 - val_loss: 1.4741 - val_accuracy: 0.7910 - val_mae: 1.9055e-04\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 1.1691 - accuracy: 0.8097 - mae: 1.7447e-04 - val_loss: 1.4293 - val_accuracy: 0.7985 - val_mae: 1.7170e-04\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 1.0957 - accuracy: 0.8144 - mae: 1.6075e-04 - val_loss: 1.4063 - val_accuracy: 0.7948 - val_mae: 1.6696e-04\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.0394 - accuracy: 0.8153 - mae: 1.5713e-04 - val_loss: 1.3968 - val_accuracy: 0.7948 - val_mae: 1.6601e-04\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.0073 - accuracy: 0.8153 - mae: 1.5624e-04 - val_loss: 1.3956 - val_accuracy: 0.7948 - val_mae: 1.6613e-04\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.9809 - accuracy: 0.8162 - mae: 1.5666e-04 - val_loss: 1.4006 - val_accuracy: 0.7948 - val_mae: 1.6628e-04\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.9480 - accuracy: 0.8172 - mae: 1.5638e-04 - val_loss: 1.4081 - val_accuracy: 0.7948 - val_mae: 1.6681e-04\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.9362 - accuracy: 0.8200 - mae: 1.5743e-04 - val_loss: 1.4152 - val_accuracy: 0.7948 - val_mae: 1.6755e-04\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.9162 - accuracy: 0.8181 - mae: 1.5735e-04 - val_loss: 1.4205 - val_accuracy: 0.7948 - val_mae: 1.6735e-04\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.9023 - accuracy: 0.8181 - mae: 1.5724e-04 - val_loss: 1.4264 - val_accuracy: 0.7985 - val_mae: 1.6668e-04\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.8944 - accuracy: 0.8200 - mae: 1.5631e-04 - val_loss: 1.4343 - val_accuracy: 0.7948 - val_mae: 1.6666e-04\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.8865 - accuracy: 0.8181 - mae: 1.5602e-04 - val_loss: 1.4413 - val_accuracy: 0.7948 - val_mae: 1.6626e-04\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.8760 - accuracy: 0.8172 - mae: 1.5589e-04 - val_loss: 1.4491 - val_accuracy: 0.7948 - val_mae: 1.6617e-04\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.8631 - accuracy: 0.8153 - mae: 1.5565e-04 - val_loss: 1.4560 - val_accuracy: 0.7948 - val_mae: 1.6637e-04\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.8554 - accuracy: 0.8190 - mae: 1.5537e-04 - val_loss: 1.4631 - val_accuracy: 0.7948 - val_mae: 1.6598e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ae49b16d90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "no_dim = 128 # # of blocks, related to the training time\n",
    "\n",
    "model = Sequential()\n",
    "# Input is: a window of 1 feature: an integer representing the word\n",
    "model.add(LSTM(no_dim, input_shape=(window, 1))) \n",
    "model.add(Dropout(0.2))\n",
    "# The output layer predicts the word, one-hot encoded (i.e. the vector is as long as the number of words)\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','mae'])\n",
    "model.fit(X_train, y_train, validation_split=0.2, batch_size=longest, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128)               66560     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2729)              352041    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 418,601\n",
      "Trainable params: 418,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 6ms/step - loss: 1.4402 - accuracy: 0.8182 - mae: 1.5227e-04\n",
      "{'loss': 1.4402086734771729, 'accuracy': 0.8181818127632141, 'mae': 0.000152268708916381}\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(X_test, y_test,return_dict = True)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To predict:  had been living together as friend and friend very mutually attached , and Emma doing just what she liked ; highly esteeming Miss Taylor ' s judgment , but directed \n",
      "Prediction:  of\n",
      "Actual word:  chiefly\n",
      "\n",
      "To predict:  She was the \n",
      "\n",
      "To predict:  It was on the wedding - day of this beloved friend that Emma first sat \n",
      "Prediction:  of\n",
      "Actual word:  in\n",
      "\n",
      "To predict:  Sixteen years had \n",
      "\n",
      "To predict:  Her mother had died too long ago for her to have more than an indistinct remembrance \n",
      "Prediction:  of\n",
      "Actual word:  of\n",
      "\n",
      "To predict:  of governess , the mildness of her temper had hardly allowed her to impose any restraint ; and the shadow of authority being now long passed away , they had \n",
      "Prediction:  of\n",
      "Actual word:  been\n",
      "\n",
      "To predict:  Sorrow came -- a gentle sorrow -- but not at all in the shape of \n",
      "Prediction:  of\n",
      "Actual word:  any\n",
      "\n",
      "To predict:  She was the youngest of the two \n",
      "Prediction:  ,\n",
      "Actual word:  daughters\n",
      "\n",
      "To predict:  It was on the wedding - day of this beloved friend that Emma first sat in mournful thought of any continuance \n",
      "Prediction:  of\n",
      "Actual word:  .\n",
      "\n",
      "To predict:  Sixteen years had Miss Taylor been in Mr . Woodhouse ' s family , less as a governess than a friend , very fond of both \n",
      "Prediction:  of\n",
      "Actual word:  daughters\n",
      "\n",
      "To predict:  The real evils , indeed , of Emma ' s situation were the power of having rather too much her own way \n",
      "Prediction:  of\n",
      "Actual word:  ,\n",
      "\n",
      "To predict:  The danger , however , was at present so unperceived , that they did not by any \n",
      "Prediction:  of\n",
      "Actual word:  means\n",
      "\n",
      "To predict:  Sorrow came -- a gentle sorrow -- but not at \n",
      "Prediction:  of\n",
      "Actual word:  all\n",
      "\n",
      "To predict:  The danger , however , was at present so unperceived , that they did not by any means \n",
      "Prediction:  of\n",
      "Actual word:  rank\n",
      "\n",
      "To predict:  Even before Miss Taylor had ceased to hold the nominal office of governess , the mildness of her temper had hardly allowed her to impose any restraint ; and the \n",
      "Prediction:  of\n",
      "Actual word:  shadow\n"
     ]
    }
   ],
   "source": [
    "no_words = 0\n",
    "for x_i, y_i in zip(X_test, y_test):\n",
    "    \n",
    "    # We need to reshape the test again\n",
    "    x = np.reshape(x_i, (1, len(x_i), 1))\n",
    "    \n",
    "    sentence = ''\n",
    "    for word in x[0]:\n",
    "        if np.sum(word) > 0:\n",
    "            sentence += reverse_dict[word[0]] + \" \"\n",
    "    if sentence != '':\n",
    "        print('\\nTo predict: ', sentence)\n",
    "    \n",
    "    prediction = model.predict(x)\n",
    "    \n",
    "    # The LSTM returns a probability for every word, we take the highest probability (argmax)\n",
    "    i_x = np.argmax(prediction)\n",
    "    i_y = np.argmax(y_i)\n",
    "    if i_x > 0:\n",
    "        print('Prediction: ', reverse_dict[i_x])\n",
    "        print('Actual word: ', reverse_dict[i_y])\n",
    "    no_words += 1\n",
    "    if no_words > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may check `keras.layers.LSTM`'s documentation for more details: \n",
    "https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "Or this Youtube video for more details on RNN model building using  `keras`\n",
    "https://www.youtube.com/watch?v=BSpXCRTOLJA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
